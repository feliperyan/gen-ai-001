{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff641a7-921d-4ba7-ab8b-145be8c2c39f",
   "metadata": {},
   "source": [
    "### Upgrade Python to 3.11 first\n",
    "\n",
    "Follow docs at https://medium.com/google-cloud/upgrade-google-vertex-ai-workbench-notebook-python-version-64ee1d8b4a9 \n",
    "\n",
    "In a terminal run the following\n",
    "\n",
    "```\n",
    "conda deactivate\n",
    "conda create -n python311 python=3.11\n",
    "conda activate python311\n",
    "conda install ipykernel\n",
    "ipython kernel install --name \"python311\" --user\n",
    "```\n",
    "\n",
    "Then back in the launcher create a new file with the new Kernel. Remember to use `%pip install...` to use the current env's Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276d5ce4-490d-4ed7-9e8c-02a30eaacd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597b7e1-f623-4030-b949-3133fd4c40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain chromadb google-cloud-aiplatform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c881602-caa1-4ad6-9d87-ccdff2f31e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "import math\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api import Collection\n",
    "from chromadb.types import Metadata\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0cba2d-87df-4805-af77-2399dff90efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "def get_docos(path:str) -> list[str]:\n",
    "    files: list[str] = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            files.append(os.path.join(path, filename))\n",
    "    \n",
    "    return files\n",
    "\n",
    "\n",
    "def split_doc(text_body:str, source:str, chunk_size:int) -> list[Document]:\n",
    "\n",
    "    overlap = math.floor(chunk_size / 10) # big assumption this is ok\n",
    "\n",
    "    splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunk_size, chunk_overlap=overlap, length_function=len)\n",
    "    result = splitter.create_documents([text_body], metadatas=[{\"source\":source}])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def insert_into_vector_db(chromaCollection:Collection, pathToDocs:str, chunk_size:int):\n",
    "    docs = get_docos(pathToDocs)\n",
    "\n",
    "    theId = 1\n",
    "\n",
    "    for doc in docs:\n",
    "        f = open(doc)\n",
    "        content = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        chunked_docs = split_doc(content, doc, chunk_size)\n",
    "        text_chunks = [x.page_content for x in chunked_docs]\n",
    "        meta = [cast(Metadata, x.metadata) for x in chunked_docs]\n",
    "        ids = [str(x) for x in range(theId, len(text_chunks)+theId)]\n",
    "        \n",
    "        chromaCollection.add(ids=ids, documents=text_chunks, metadatas=meta)\n",
    "\n",
    "        theId += len(text_chunks)\n",
    "\n",
    "\n",
    "def ask_question(question:str, chromaCollection:Collection, model:TextGenerationModel, info=False):\n",
    "    print(f\"{bcolors.OKCYAN}Asking...{bcolors.ENDC}\\n\")\n",
    "\n",
    "    query_result = chromaCollection.query(query_texts=[question], n_results=3)\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": 0.4,\n",
    "        \"max_output_tokens\": 256,\n",
    "        \"top_p\": 0.79,\n",
    "        \"top_k\": 40\n",
    "    }\n",
    "\n",
    "    if info:\n",
    "        print(f\"Using temp: {parameters['temperature']} | top_p: {parameters['top_p']} | top_k: {parameters['top_k']} | max_out_tokens: {parameters['max_output_tokens']}\\n\")\n",
    "        print(\"Vector DB similarity results:\\n\")\n",
    "        print(query_result)\n",
    "        print('\\n')\n",
    "\n",
    "    if query_result['documents'] is not None:\n",
    "        context_string = \" \".join(query_result['documents'][0])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        {context_string} \n",
    "        \n",
    "        {question} \n",
    "        \"\"\"\n",
    "\n",
    "        response = model.predict(prompt, **parameters)\n",
    "        print(f'{bcolors.OKCYAN}Response:{bcolors.ENDC}\\n\\n{response.text}')\n",
    "\n",
    "\n",
    "def ask_question_chronological_context(question:str, chromaCollection:Collection, model:TextGenerationModel, info=False, n_results=3, surrounding=1):\n",
    "    ''' Similar to ask_question but after getting the most relevant documents from ChromaDB, it takes advantage of the sequential nature of the ID and also\n",
    "        gets the surrounding documents. Ie: If I ask about \"steam\" the document immediatelly before and after the one containing the word steam may be highly\n",
    "        relevant.\n",
    "\n",
    "        This is highly coupled to ChromaDB. I'm unsure whether other VectorDBs allow to be queried by ID, etc\n",
    "    '''\n",
    "    \n",
    "    print(f\"{bcolors.OKCYAN}Asking...{bcolors.ENDC}\\n\")\n",
    "\n",
    "    query_result = chromaCollection.query(query_texts=[question], n_results=n_results)\n",
    "    ids = [int(x) for x in query_result['ids'][0]]\n",
    "    ids.sort()\n",
    "\n",
    "    all_ids = []\n",
    "    for id in ids:\n",
    "        for i in range(1, surrounding+1):\n",
    "            pos = id - (surrounding + 1 - i)\n",
    "            all_ids.append(str(pos))\n",
    "\n",
    "        all_ids.append(str(id))\n",
    "        \n",
    "        for i in range(1, surrounding+1):\n",
    "            all_ids.append(str(id+i))\n",
    "    \n",
    "\n",
    "    query_result = chromaCollection.get(ids=all_ids)\n",
    "\n",
    "    parameters = {\n",
    "        \"temperature\": 0.4,\n",
    "        \"max_output_tokens\": 256,\n",
    "        \"top_p\": 0.79,\n",
    "        \"top_k\": 40\n",
    "    }\n",
    "\n",
    "    if info:\n",
    "        print(f\"Using temp: {parameters['temperature']} | top_p: {parameters['top_p']} | top_k: {parameters['top_k']} | max_out_tokens: {parameters['max_output_tokens']}\\n\")\n",
    "        print(\"Vector DB similarity results:\\n\")\n",
    "        print(query_result)\n",
    "        print('\\n')\n",
    "\n",
    "    if query_result['documents'] is not None:\n",
    "        context_string = \" \".join(query_result['documents'][0])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        {context_string} \n",
    "        \n",
    "        {question} \n",
    "        \"\"\"\n",
    "\n",
    "        response = model.predict(prompt, **parameters)\n",
    "        print(f'{bcolors.OKCYAN}Response:{bcolors.ENDC}\\n\\n{response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef70c59-90a3-49f1-a9cb-3870466521d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vertexai...\n",
      "Initializing vertexai... \u001b[92mDONE\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing vertexai...\")\n",
    "PROJECT_ID = \"fryan-crdemo-1\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "generation_model = cast(TextGenerationModel, TextGenerationModel.from_pretrained(\"text-bison@001\"))\n",
    "print(f\"Initializing vertexai... {bcolors.OKGREEN}DONE{bcolors.ENDC}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35767b3f-7cbc-4255-a283-5e82c415f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing docs into Vector DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100% 79.3M/79.3M [00:07<00:00, 10.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing docs into Vector DB... \u001b[92mDONE\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{bcolors.WARNING}If running for the first time, ChromaDB will download a small text model to extract embeddings, this may take a minute.{bcolors.ENDC}\\n')\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"collection_ancient\")\n",
    "print(\"Parsing docs into Vector DB...\")\n",
    "insert_into_vector_db(collection, \"./docs\", 300)\n",
    "print(f\"Parsing docs into Vector DB... {bcolors.OKGREEN}DONE{bcolors.ENDC}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f45a08-0eb5-4659-8599-4fa889f8360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mQuestion:\u001b[0m Which ancient civilization was the most militarized?\n",
      "\n",
      "\u001b[96mAsking...\u001b[0m\n",
      "\n",
      "\u001b[96mResponse:\u001b[0m\n",
      "\n",
      "The ancient Roman civilization was the most militarized civilization in the world. The Romans had a highly developed military system that was based on a strong infantry force. The Roman army was also well-equipped with weapons and armor. The Romans were able to conquer a large empire because of their military strength.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which ancient civilization was the most militarized?\"\n",
    "print(f\"{bcolors.OKCYAN}Question:{bcolors.ENDC} {question}\\n\")\n",
    "ask_question(question, collection, generation_model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3530b0a-f9fc-4f11-a741-a6f4ef2188f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311 (Local)",
   "language": "python",
   "name": "local-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
